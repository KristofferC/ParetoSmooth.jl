{
    "sourceFile": "src/Bootstrap.jl",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1627341840659,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1627341840659,
            "name": "Commit-0",
            "content": "using AxisKeys\nusing MeasureTheory\nusing InteractiveUtils\nusing LoopVectorization\nusing Statistics\nusing Tullio\n\nexport bayes_val\n\n\"\"\"\n    function bayes_cv(\n        log_likelihood::Array{Float} [, args...];\n        source::String=\"mcmc\" [, chain_index::Vector{Int}, kwargs...]\n    ) -> PsisBB\n\nUse the Bayesian bootstrap (Bayes cross-validation) and PSIS to calculate an approximate\nposterior for the out-of-sample score.\n\n\n# Arguments\n\n  - `log_likelihood::Array`: An array or matrix of log-likelihood values indexed as\n    `[data, step, chain]`. The chain argument can be left off if `chain_index` is provided\n    or if all posterior samples were drawn from a single chain.\n  - `args...`: Positional arguments to be passed to [`psis`](@ref).\n  - `chain_index::Vector`: An (optional) vector of integers specifying which chain each\n    step belongs to. For instance, `chain_index[3]` should return `2` if\n    `log_likelihood[:, 3]` belongs to the second chain.\n  - `kwargs...`: Keyword arguments to be passed to [`psis`](@ref).\n\n\n# Extended help\nThe Bayesian bootstrap works similarly to other cross-validation methods: First, we remove\nsome piece of information from the model. Then, we test how well the model can reproduce \nthat information. With leave-k-out cross validation, the information we leave out is the\nvalue for one or more data points. With the Bayesian bootstrap, the information being left\nout is the true probability of each observation.\n\n\nSee also: [`BayesCV`](@ref), [`psis`](@ref), [`loo`](@ref), [`PsisLoo`](@ref).\n\"\"\"\nfunction bayes_cv(\n    log_likelihood::T, \n    args...;\n    resamples::Integer=2^10, \n    rng=MersenneTwister(1776),\n    kwargs...\n) where {F<:AbstractFloat, T<:AbstractArray{F, 3}}\n\n    dims = size(log_likelihood)\n    data_size = dims[1]\n    mcmc_count = dims[2] * dims[3]  # total number of samples from posterior\n    log_count = log(mcmc_count)\n\n\n    # TODO: Add a way of using score functions other than ELPD\n    bb_weights = data_size * rand(rng, Dirichlet(ones(data_size)), resamples)\n    bb_samples = similar(log_likelihood, (resamples, data_size))\n    psis_object = psis(bb_samples, args...; kwargs...)\n\n    # \"Pointwise\" used here to refer to \"per resample\"\n    @tullio pointwise_naive[i] := log <|\n        psis_object.weights[i, j, k] * exp(bb_samples[i, j, k])\n    @tullio sample_est := exp(log_likelihood[i, j, k] - log_count)\n    sample_est = log(sample_est)\n    \n    @tturbo bb_ests .= (2 * sample_est) .- pointwise_naive\n    @tullio pointwise_mcse[i] :=  # I'll take sqrt later in-place\n        (weights[i, j, k] * (log_likelihood[i, j, k] - pointwise_loo[i]))^2\n    # Apply law of total variance\n    bootstrap_se = var(naive_ests) / bb_samples\n    mcse = sqrt(mean(pointwise_mcse) + bootstrap_se)\n    @tturbo @. pointwise_mcse = sqrt(pointwise_mcse)\n        \n    # Posterior for the *average score*, not the mean of the posterior distribution:\n    posterior_avg = bb_ests / data_size\n    resample_calcs = KeyedArray(\n        hcat(\n            bb_ests,\n            pointwise_naive,\n            pointwise_overfit,\n            pointwise_mcse,\n            psis_object.pareto_k\n        );\n        data=1:length(pointwise_loo),\n        statistic=[\n            :loo_est,\n            :naive_est,\n            :overfit,\n            :mcse,\n            :pareto_k\n        ],\n    )\n\n    estimates = _generate_bayes_table(log_likelihood, resample_calcs, data_size)\n\n    return BayesCV(\n        estimates,\n        resample_calcs,\n        psis_object\n    )\n\nend\n\n\nfunction bayes_cv(\n    log_likelihood::T,\n    args...;\n    chain_index::AbstractVector=ones(size(log_likelihood, 1)),\n    kwargs...,\n) where {F <: AbstractFloat, T <: AbstractMatrix{F}}\n    new_log_ratios = _convert_to_array(log_likelihood, chain_index)\n    return psis_loo(new_log_ratios, args...; kwargs...)\nend\n\n\nfunction _generate_bayes_table(\n    log_likelihood::AbstractArray, \n    pointwise::AbstractArray, \n    data_size::Integer\n)\n\n    # create table with the right labels\n    table = KeyedArray(\n        similar(log_likelihood, 3, 4);\n        criterion=[:cv_est, :naive_est, :overfit],\n        statistic=[:ev_total, :se_total, :ev_mean, :se_mean, :sd_mean],\n    )\n    \n    # calculate the sample expectation for the total score\n    to_sum = pointwise([:loo_est, :naive_est])\n    @tullio total[crit] := to_sum[data, crit]\n    table(:, :total) .= reshape(total, 3)\n\n    # calculate the sample expectation for the average score\n    table(:, :mean) .= table(:, :total) ./ data_size\n\n    # calculate the sample expectation for the standard error in the totals\n    @_ table(:, :se_total) .= pointwise([:loo_est, :naive_est, :overfit]) |> \n        varm(_, table(:, :mean); dims=1) |>\n        sqrt.(data_size * _) |>\n        reshape(_, 3)\n\n    # calculate the sample expectation for the standard error in averages\n    table(:, :se_mean) .= table(:, :se_total) ./ data_size\n\n    return table\nend\n"
        }
    ]
}